{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 7 Penguins\n",
    "\n",
    "#### Pablo Carmona Artal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "species              0\n",
       "island               0\n",
       "bill_length_mm       0\n",
       "bill_depth_mm        0\n",
       "flipper_length_mm    0\n",
       "body_mass_g          0\n",
       "sex                  0\n",
       "year                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "penguins_data = pd.read_csv(\"/workspaces/Modelos_IA/penguins.csv\")\n",
    "#Con esto vemos si hubiese algun NaN que tuviesemos que tratar \n",
    "penguins_data.isna().sum()\n",
    "#Vemos que hay entonces propongo eliminarlos ya que no nos van a proporcionar informacion\n",
    "penguins_data = penguins_data.dropna(subset='sex')\n",
    "penguins_data.isna().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>male</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.3</td>\n",
       "      <td>20.6</td>\n",
       "      <td>190.0</td>\n",
       "      <td>3650.0</td>\n",
       "      <td>male</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>Chinstrap</td>\n",
       "      <td>Dream</td>\n",
       "      <td>55.8</td>\n",
       "      <td>19.8</td>\n",
       "      <td>207.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>male</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>Chinstrap</td>\n",
       "      <td>Dream</td>\n",
       "      <td>43.5</td>\n",
       "      <td>18.1</td>\n",
       "      <td>202.0</td>\n",
       "      <td>3400.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>Chinstrap</td>\n",
       "      <td>Dream</td>\n",
       "      <td>49.6</td>\n",
       "      <td>18.2</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3775.0</td>\n",
       "      <td>male</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>Chinstrap</td>\n",
       "      <td>Dream</td>\n",
       "      <td>50.8</td>\n",
       "      <td>19.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>4100.0</td>\n",
       "      <td>male</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>Chinstrap</td>\n",
       "      <td>Dream</td>\n",
       "      <td>50.2</td>\n",
       "      <td>18.7</td>\n",
       "      <td>198.0</td>\n",
       "      <td>3775.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>333 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0       Adelie  Torgersen            39.1           18.7              181.0   \n",
       "1       Adelie  Torgersen            39.5           17.4              186.0   \n",
       "2       Adelie  Torgersen            40.3           18.0              195.0   \n",
       "4       Adelie  Torgersen            36.7           19.3              193.0   \n",
       "5       Adelie  Torgersen            39.3           20.6              190.0   \n",
       "..         ...        ...             ...            ...                ...   \n",
       "339  Chinstrap      Dream            55.8           19.8              207.0   \n",
       "340  Chinstrap      Dream            43.5           18.1              202.0   \n",
       "341  Chinstrap      Dream            49.6           18.2              193.0   \n",
       "342  Chinstrap      Dream            50.8           19.0              210.0   \n",
       "343  Chinstrap      Dream            50.2           18.7              198.0   \n",
       "\n",
       "     body_mass_g     sex  year  \n",
       "0         3750.0    male  2007  \n",
       "1         3800.0  female  2007  \n",
       "2         3250.0  female  2007  \n",
       "4         3450.0  female  2007  \n",
       "5         3650.0    male  2007  \n",
       "..           ...     ...   ...  \n",
       "339       4000.0    male  2009  \n",
       "340       3400.0  female  2009  \n",
       "341       3775.0    male  2009  \n",
       "342       4100.0    male  2009  \n",
       "343       3775.0  female  2009  \n",
       "\n",
       "[333 rows x 8 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penguins_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividimos los datos en X e Y del entrenamiento\n",
    "Y_penguins = penguins_data['species'] #target\n",
    "X_penguins = penguins_data[penguins_data.columns[1:].values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adelie', 'Chinstrap', 'Gentoo'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tres especies de nuesto ouput\n",
    "set(Y_penguins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(263, 7)\n",
      "(70, 7)\n"
     ]
    }
   ],
   "source": [
    "#Dividimos el train y validacion en 80% y 20% respectivamente\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_penguins, Y_penguins, test_size=70, random_state=123,\n",
    "                                                                        shuffle=True, stratify=Y_penguins)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Biscoe' 'Dream' 'Torgersen']\n",
      "[0 1 2]\n",
      "['female' 'male']\n",
      "[0 1]\n",
      "['Adelie' 'Chinstrap' 'Gentoo']\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "#Estandarizamos los datos para manatener los datos y el error regularizado \n",
    "penguins_data.dtypes\n",
    "#El problema es que hay datos no numericos, los cuales no se pueden normalizar, y tampoco analizar por un modelo de redes neuronales.\n",
    "#Por ello dividimos los datos numericos para poder estandarizarlos\n",
    "num_values = X_train.select_dtypes(include=\"float64\").columns\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[num_values].values)\n",
    "X_train[num_values] = scaler.transform(X_train[num_values].values)\n",
    "X_test[num_values] = scaler.transform(X_test[num_values].values)\n",
    "\n",
    "#A continuacion hacemos la codificacion de los valores categoricos \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#Creamos un label encoder para cada uno\n",
    "#Para Island\n",
    "island_label= LabelEncoder()\n",
    "island_label.fit(X_train['island'].values)\n",
    "#Comprobamos que la transfromacion sea adecuada\n",
    "print(island_label.classes_)\n",
    "print(island_label.transform(island_label.classes_))\n",
    "#y transformamos\n",
    "X_train['island'] = island_label.transform(X_train['island'].values)\n",
    "X_test['island'] = island_label.transform(X_test['island'].values)\n",
    "#Para el genero\n",
    "Gender_label = LabelEncoder()\n",
    "Gender_label.fit(X_train['sex'].values)\n",
    "print(Gender_label.classes_)\n",
    "print(Gender_label.transform(Gender_label.classes_))\n",
    "X_train['sex'] = Gender_label.transform(X_train['sex'].values)\n",
    "X_test['sex'] = Gender_label.transform(X_test['sex'].values)\n",
    "\n",
    "\n",
    "#Para el target (species)\n",
    "target_label = LabelEncoder()\n",
    "target_label.fit(y_train)\n",
    "print(target_label.classes_)\n",
    "print(target_label.transform(target_label.classes_))\n",
    "y_train = target_label.transform(y_train)\n",
    "y_test = target_label.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.975967</td>\n",
       "      <td>-0.544048</td>\n",
       "      <td>-1.597313</td>\n",
       "      <td>-1.641909</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>1</td>\n",
       "      <td>1.006126</td>\n",
       "      <td>0.517336</td>\n",
       "      <td>-0.545678</td>\n",
       "      <td>-0.547105</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0</td>\n",
       "      <td>0.789246</td>\n",
       "      <td>-1.302180</td>\n",
       "      <td>0.856503</td>\n",
       "      <td>2.049145</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>1</td>\n",
       "      <td>1.313372</td>\n",
       "      <td>1.022757</td>\n",
       "      <td>-0.545678</td>\n",
       "      <td>-0.703505</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>0</td>\n",
       "      <td>0.500074</td>\n",
       "      <td>-1.453806</td>\n",
       "      <td>0.996721</td>\n",
       "      <td>0.797940</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.530104</td>\n",
       "      <td>0.466794</td>\n",
       "      <td>0.295631</td>\n",
       "      <td>0.109778</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>1</td>\n",
       "      <td>0.445854</td>\n",
       "      <td>0.365710</td>\n",
       "      <td>-0.615787</td>\n",
       "      <td>-0.891186</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0</td>\n",
       "      <td>0.445854</td>\n",
       "      <td>-1.858143</td>\n",
       "      <td>0.646176</td>\n",
       "      <td>0.422579</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.457811</td>\n",
       "      <td>0.668962</td>\n",
       "      <td>0.015195</td>\n",
       "      <td>-0.265584</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0</td>\n",
       "      <td>0.156681</td>\n",
       "      <td>-1.959227</td>\n",
       "      <td>0.856503</td>\n",
       "      <td>1.110741</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>263 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     island  bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
       "98        1       -1.975967      -0.544048          -1.597313    -1.641909   \n",
       "341       1        1.006126       0.517336          -0.545678    -0.547105   \n",
       "165       0        0.789246      -1.302180           0.856503     2.049145   \n",
       "278       1        1.313372       1.022757          -0.545678    -0.703505   \n",
       "272       0        0.500074      -1.453806           0.996721     0.797940   \n",
       "..      ...             ...            ...                ...          ...   \n",
       "91        1       -0.530104       0.466794           0.295631     0.109778   \n",
       "276       1        0.445854       0.365710          -0.615787    -0.891186   \n",
       "157       0        0.445854      -1.858143           0.646176     0.422579   \n",
       "151       1       -0.457811       0.668962           0.015195    -0.265584   \n",
       "200       0        0.156681      -1.959227           0.856503     1.110741   \n",
       "\n",
       "     sex  year  \n",
       "98     0  2008  \n",
       "341    1  2009  \n",
       "165    1  2007  \n",
       "278    1  2007  \n",
       "272    0  2009  \n",
       "..   ...   ...  \n",
       "91     1  2008  \n",
       "276    0  2007  \n",
       "157    0  2007  \n",
       "151    1  2009  \n",
       "200    0  2008  \n",
       "\n",
       "[263 rows x 7 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento (media no std): island               6.501901e-01\n",
      "bill_length_mm       1.160880e-16\n",
      "bill_depth_mm       -8.105050e-17\n",
      "flipper_length_mm   -7.564714e-16\n",
      "body_mass_g         -4.930572e-16\n",
      "sex                  5.019011e-01\n",
      "year                 2.008019e+03\n",
      "dtype: float64\n",
      "Entrenamiento (media no std): island                  0.657143\n",
      "bill_length_mm         -0.034638\n",
      "bill_depth_mm          -0.027797\n",
      "flipper_length_mm       0.061266\n",
      "body_mass_g            -0.030983\n",
      "sex                     0.514286\n",
      "year                 2008.128571\n",
      "dtype: float64\n",
      "Entrenamiento: (desv. tipica std) island               0.713310\n",
      "bill_length_mm       1.000000\n",
      "bill_depth_mm        1.000000\n",
      "flipper_length_mm    1.000000\n",
      "body_mass_g          1.000000\n",
      "sex                  0.499996\n",
      "year                 0.815499\n",
      "dtype: float64\n",
      "Entrenamiento: (desv. tipica std) island               0.714857\n",
      "bill_length_mm       0.935454\n",
      "bill_depth_mm        0.969825\n",
      "flipper_length_mm    0.905224\n",
      "body_mass_g          1.027748\n",
      "sex                  0.499796\n",
      "year                 0.791408\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Comporbamos que los datos se hayan normalizado adecuadamente\n",
    "print(\"Entrenamiento (media no std): {}\".format(np.mean(X_train, axis=0)))\n",
    "print(\"Entrenamiento (media no std): {}\".format(np.mean(X_test, axis=0)))\n",
    "print(\"Entrenamiento: (desv. tipica std) {}\".format(np.std(X_train, axis=0)))\n",
    "print(\"Entrenamiento: (desv. tipica std) {}\".format(np.std(X_test, axis=0)))\n",
    "#vemos como efectivamente las medias de los valores numericos tranformados estan en torno a 0 y las desviaciones estandar en torno a 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nos aseguramos de coger tan solo los values\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usamos dos variables explicativas como recomienda el enunciado\n",
    "tX_train = torch.tensor(X_train, requires_grad=False, dtype=torch.float)[:, 0:2]\n",
    "tX_test = torch.tensor(X_test, requires_grad=False, dtype=torch.float)[:, 0:2]\n",
    "ty_train = torch.tensor(y_train, requires_grad=False, dtype=torch.long)  \n",
    "ty_test = torch.tensor(y_test, requires_grad=False, dtype=torch.long) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([263, 2])\n",
      "torch.Size([70, 2])\n",
      "torch.Size([263])\n",
      "torch.Size([70])\n"
     ]
    }
   ],
   "source": [
    "#Observamos como las medidad concuerdan\n",
    "print(tX_train.shape)\n",
    "print(tX_test.shape)\n",
    "print(ty_train.shape)\n",
    "print(ty_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuacion comenzaremos con la construccion de la red neuronal. En nuesto caso se compondra de una red de 7 neuronas de entrada y tres de salida, la red nueronal nos dara la probabilidad de que se pertenezca a cualquiera de las tres especies {'Adelie', 'Chinstrap', 'Gentoo'}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construimos nuestra red neuronal con 80 neuronas en nuestra capa intermedia y como funcion de activacion de esta capa usaremos la funcion Relu\n",
    "\n",
    "#Con la funcion Sequential somos capaces de encadenar procesos, y pasan al siguiente modulo a traves de fowards\n",
    "nn1 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(tX_train.shape[1], 80), # capa lineal con 7 entradas y 80 salidas\n",
    "    torch.nn.ReLU(),  # activaciÃ³n ReLU\n",
    "    torch.nn.Linear(80, len(target_label.classes_)), # capa lineal con 80 entradas y 3 salidas. \n",
    "    torch.nn.LogSoftmax(dim=1) # El output es la probabilidad de pertenencia a cada categorÃ­a (class)\n",
    ")\n",
    "#A traves dela funcion Softmax (o LogSoftMax), nos permiten saber las probabilidades de que pertenezcan a una variable categorica especifica.\n",
    "#Usamos LogSoftMax porque devuelve unas probabilidades mas estables que el original. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tensor no tiene valores NaN.\n"
     ]
    }
   ],
   "source": [
    "#Comprobacion por NaNs\n",
    "tiene_nan = torch.isnan(tX_train).any().item()\n",
    "if tiene_nan:\n",
    "    print(\"El tensor tiene al menos un valor NaN.\")\n",
    "else:\n",
    "    print(\"El tensor no tiene valores NaN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1954, 0.3291, 0.4755],\n",
      "        [0.3080, 0.3234, 0.3686],\n",
      "        [0.3516, 0.2621, 0.3863],\n",
      "        [0.3200, 0.3137, 0.3663],\n",
      "        [0.3463, 0.2705, 0.3833]], grad_fn=<ExpBackward0>)\n",
      "tensor([0, 1, 2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "print(torch.exp(nn1(tX_train[0:5,:])))\n",
    "print(ty_train[0:5])\n",
    "#Vemos como al usar todas las variables como el modelo nos da probabilidades erroneas, para ello he pensado en reducir el numero de variables que incluimos en el analisis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2159, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculamos el valor de perdidas del modelo \n",
    "#Para las 50 primeros casos. Ejemplo\n",
    "torch.nn.functional.nll_loss(nn1(tX_train), ty_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A continuacion creamos las semillas iniciales que genera numeros aleatorios, para que sea punto de partida para la creacion de valores iniciales para nuestros parametros usados en el entrenamiento\n",
    "#Hay varias formas de inicializar los pesos, en nuestro caso al ser una red nuronal simple los iniciaremos con un numero aleatorio entre 1 y 0 \n",
    "def do_seeds(sn):\n",
    "    np.random.seed(sn)\n",
    "    torch.manual_seed(sn)\n",
    "    random.seed(sn)\n",
    "do_seeds(0)\n",
    "\n",
    "do_seeds(12)\n",
    "for param in nn1.parameters():\n",
    "    torch.nn.init.uniform_(param, 0, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Una vez inicializados los parametros, podemo iniciar el entrenamiento, donde buscaremos reducir el error, a traves del Gradiente Descendiente. \n",
    "#Hay dos tipos el estandard y el escolastico, el cual usaremos por su eficacia y rapidez.\n",
    "\n",
    "opt = torch.optim.SGD(nn1.parameters(), lr=0.02) #En este caso usaremos un learning rate del 0.02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ã‰poca: 0, PÃ©rdida: 2.2929\n",
      "Ã‰poca: 1000, PÃ©rdida: 1.9003\n",
      "Ã‰poca: 2000, PÃ©rdida: 1.5535\n",
      "Ã‰poca: 3000, PÃ©rdida: 1.2724\n",
      "Ã‰poca: 4000, PÃ©rdida: 1.0865\n",
      "Ã‰poca: 5000, PÃ©rdida: 0.9831\n",
      "Ã‰poca: 6000, PÃ©rdida: 0.9187\n",
      "Ã‰poca: 7000, PÃ©rdida: 0.8695\n",
      "Ã‰poca: 8000, PÃ©rdida: 0.8282\n",
      "Ã‰poca: 9000, PÃ©rdida: 0.7925\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):  \n",
    "    pred_y = nn1(tX_train) # calculamos nuestras predicciones con los parÃ¡metros actuales\n",
    "    loss = torch.nn.functional.nll_loss(pred_y, ty_train) # calculamos la pÃ©rdida con los resultados \n",
    "    \n",
    "    if i%1000 == 0:\n",
    "        print(\"Ã‰poca: {}, PÃ©rdida: {:.4f}\".format(i, loss.item()))\n",
    "        loss.backward() # propagamos las derivadas de la 'loss' con respecto a nuestros parÃ¡metros\n",
    "        opt.step() # ejecutamos un paso de nuestro optimizador\n",
    "        opt.zero_grad() # reseteamos los valores acumulados de las derivadas para que no las acumule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos como ha reducido la perdida bastante gracias al uso del Gradiente Descendente Escolastico "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 1, 2, 0, 0, 0, 2, 0])\n",
      "tensor([0, 1, 2, 1, 2, 0, 0, 0, 2, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2122/2424509203.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_t = torch.tensor(x, dtype=torch.float, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "#Ahora ya tenemos nuestra red entrenada, por lo que podemos observar realizar predicciones con certeza\n",
    "\n",
    "#Usamos previamente requires_grad to liberar espacio\n",
    "for param in nn1.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#Continuamos creando una funcion donde demos datos en forma torch y un modelo, y que nos de los valores numericos que tienen la mayor probabilidad (predicciones)\n",
    "def pred_f(x, forw):\n",
    "    x_t = torch.tensor(x, dtype=torch.float, requires_grad=False)\n",
    "    _,y_pred = torch.max(forw(x_t), dim=1)\n",
    "    return y_pred\n",
    "\n",
    "#Probamos con las 10 primeras filas\n",
    "print(pred_f(tX_train[0:10,:],nn1))\n",
    "print(ty_train[0:10])\n",
    "#Unicamente falla en el primero, pero por ahora se ve bastante preciso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2122/1969592921.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_label = torch.tensor(y, dtype=torch.long, requires_grad=False)   # recupera el valor histÃ³rico de y\n",
      "/tmp/ipykernel_2122/2424509203.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_t = torch.tensor(x, dtype=torch.float, requires_grad=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.844106463878327"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creamos una funcion de score donde nos de el % de aciertos comparado con la y extraida al principio\n",
    "def score_f(x, y, forw):\n",
    "    y_label = torch.tensor(y, dtype=torch.long, requires_grad=False)   # recupera el valor histÃ³rico de y\n",
    "    y_pred = pred_f(x, forw)                                           # obtiene la predicciÃ³n de y\n",
    "    \n",
    "    # devuelve el % aciertos (cuando y coincide con y_pred)\n",
    "    return torch.sum(y_pred == y_label).item() / len(y_label)  \n",
    "\n",
    "score_f(tX_train,ty_train,nn1)\n",
    "#Nos da un porcetaje de precision del 87% lo cual es muy positivo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2122/1969592921.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_label = torch.tensor(y, dtype=torch.long, requires_grad=False)   # recupera el valor histÃ³rico de y\n",
      "/tmp/ipykernel_2122/2424509203.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_t = torch.tensor(x, dtype=torch.float, requires_grad=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8142857142857143"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Probamos la validacion para asegurarnos que tambien sea estable \n",
    "score_f(tX_test,ty_test,nn1)\n",
    "#Una precision muy similar y tambien elevada, es verdad que no se obtienen los mismos resulados entre el test y el entrenamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2122/1969592921.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_label = torch.tensor(y, dtype=torch.long, requires_grad=False)   # recupera el valor histÃ³rico de y\n",
      "/tmp/ipykernel_2122/2424509203.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_t = torch.tensor(x, dtype=torch.float, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "Results = pd.DataFrame({\n",
    "    'model':['Original'],\n",
    "    'train': [score_f(tX_train,ty_train,nn1)],\n",
    "    'test': [score_f(tX_test,ty_test,nn1)]\n",
    "})\n",
    "Results.set_index('model',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ã‰poca: 0, PÃ©Å•dida: 53.4568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ã‰poca: 1000, PÃ©Å•dida: 0.2753\n",
      "Ã‰poca: 2000, PÃ©Å•dida: 0.1212\n",
      "Ã‰poca: 3000, PÃ©Å•dida: 0.1145\n",
      "Ã‰poca: 4000, PÃ©Å•dida: 0.1185\n",
      "Ã‰poca: 5000, PÃ©Å•dida: 0.1218\n",
      "Ã‰poca: 6000, PÃ©Å•dida: 0.1257\n",
      "Ã‰poca: 7000, PÃ©Å•dida: 0.1286\n",
      "Ã‰poca: 8000, PÃ©Å•dida: 0.1305\n",
      "Ã‰poca: 9000, PÃ©Å•dida: 0.1307\n"
     ]
    }
   ],
   "source": [
    "#Podriamos itentar continuar mejorando el modelo a traves de la contruccion de una red mas compleja, con otra capa intermedia y esta vez usando 200 unidades, y luego otra de 100.\n",
    "nn2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(tX_train.shape[1], 200), \n",
    "    torch.nn.ReLU(), \n",
    "    torch.nn.Linear(200, 100), \n",
    "    torch.nn.ReLU(), \n",
    "    torch.nn.Linear(100, len(target_label.classes_)), \n",
    "    torch.nn.LogSoftmax(dim=1) # El output es la probabilidad de pertenencia a cada categorÃ­a\n",
    ")\n",
    "\n",
    "# reinicializamos los parÃ¡metros\n",
    "do_seeds(12)\n",
    "for param in nn2.parameters():\n",
    "    torch.nn.init.uniform_(param, 0, 1) \n",
    "\n",
    "#En este caso aparte de estimar un learning rate para el modelo Gradiente Escolastico Descendiente, se le aÃ±ade el regularizador Weight_decay, el cual castiga la perdida\n",
    "opt2 = torch.optim.SGD(nn2.parameters(), lr=0.02, weight_decay=0.02)\n",
    "\n",
    "for i in range(10000):\n",
    "    opt2.zero_grad() # reseteamos los valores acumulados de las derivadas\n",
    "    pred_y = nn2(tX_train) # calculamos nuestras predicciones con los parÃ¡metros actuales\n",
    "    loss = torch.nn.functional.nll_loss(pred_y, ty_train) # calculamos la 'loss' con los resultados \n",
    "    if i%1000 == 0:\n",
    "        print(\"Ã‰poca: {}, PÃ©Å•dida: {:.4f}\".format(i, loss.item()))\n",
    "    loss.backward() # propagamos las derivadas de la 'loss' con respecto a nuestros parÃ¡metros\n",
    "    opt2.step() # ejecutamos un paso de nuestro optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9581749049429658\n",
      "0.9285714285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2122/1969592921.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_label = torch.tensor(y, dtype=torch.long, requires_grad=False)   # recupera el valor histÃ³rico de y\n",
      "/tmp/ipykernel_2122/2424509203.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_t = torch.tensor(x, dtype=torch.float, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "for param in nn2.parameters():\n",
    "    param.requires_grad = False\n",
    "print(score_f(tX_train, ty_train, nn2))\n",
    "\n",
    "print(score_f(tX_test, ty_test, nn2))\n",
    "#Lo cual lo hace incluso mas preciso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2122/1969592921.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_label = torch.tensor(y, dtype=torch.long, requires_grad=False)   # recupera el valor histÃ³rico de y\n",
      "/tmp/ipykernel_2122/2424509203.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_t = torch.tensor(x, dtype=torch.float, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "Modelo_complejo = {'model':['Modelo_Complejo'],'train':[score_f(tX_train, ty_train, nn2)],'test':[score_f(tX_test, ty_test, nn2)]}\n",
    "Results.loc['Modelo_complejo'] = Modelo_complejo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2122/1969592921.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_label = torch.tensor(y, dtype=torch.long, requires_grad=False)   # recupera el valor histÃ³rico de y\n",
      "/tmp/ipykernel_2122/2424509203.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_t = torch.tensor(x, dtype=torch.float, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "Modelo_complejo = {'model':['Modelo_Complejo'],'train':score_f(tX_train, ty_train, nn2),'test':score_f(tX_test, ty_test, nn2)}\n",
    "Results.loc['Modelo_complejo'] = Modelo_complejo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.844106</td>\n",
       "      <td>0.814286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Modelo_complejo</th>\n",
       "      <td>0.958175</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    train      test\n",
       "model                              \n",
       "Original         0.844106  0.814286\n",
       "Modelo_complejo  0.958175  0.928571"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
